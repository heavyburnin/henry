# ğŸ§  Henry â€” Local AI Chatbot using LLaMA.cpp

**Henry** is a fast, local-first AI chatbot built with Flask and [`llama.cpp`](https://github.com/ggerganov/llama.cpp). It runs quantized LLMs on your machine using the GGUF format â€” no cloud APIs, no data leaving your device.

![Screenshot](screenshot.png)

---

## âœ¨ Features

- ğŸƒâ€â™‚ï¸ Runs fully offline with quantized `.gguf` models
- ğŸ§  Powered by [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- ğŸ’¬ Flask web UI with real-time chat
- ğŸ”„ Easily swap LLM models (like TinyLLaMA or Zephyr)
- ğŸ¨ Clean and customizable frontend

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/heavyburnin/henry.git
cd henry-chatbot
```

### 2. Set up a Python virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Download and build `llama.cpp`

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build && cd build
cmake ..
cmake --build . --config Release
cd ../..
```

### 4. Download a model (GGUF format)

Pick a quantized `.gguf` model from Hugging Face:

- [TinyLLaMA](https://huggingface.co/)
- [Zephyr](https://huggingface.co/)

Put it in the `models/` folder:

```
henry-chatbot/
â””â”€â”€ models/
    â””â”€â”€ zephyr-7b-alpha.Q2_K.gguf
```

### 5. Run the app

```bash
python app.py
```

Visit: [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸ§  Tips & Troubleshooting

- â„ï¸ **App stuck loading after POST?** Your model might be too large. Try a smaller quant like `Q2_K`.
- ğŸ”  **Getting `<UNK>` tokens?** Your tokenizer may not match the model â€” prefer chat-tuned GGUFs.
- ğŸ¢ **Slow inference?** Try disabling debug mode or reducing `n_ctx` in `model_runner.py`.

---

## âš™ï¸ Requirements

- Python 3.8+
- CMake 3.16+
- A C++17-compatible compiler (GCC, Clang, etc.)
- 4â€“8 GB RAM (minimum for smaller models)

---

## ğŸ“œ License

This project is licensed under the MIT License.  
Use at your own risk, and always check licenses of the models you download.

---

## ğŸ™ Acknowledgements

- [llama.cpp](https://github.com/ggerganov/llama.cpp) by Georgi Gerganov
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- Hugging Face model authors for open LLMs

---

ğŸ’¬ Feel free to open issues or submit a PR!
